
import { useState, useEffect, useRef } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useToast } from '@/hooks/use-toast';

export interface ScrapingLog {
  id: string;
  source: string;
  status: 'running' | 'completed' | 'failed';
  items_scraped: number;
  items_added: number;
  items_updated: number;
  items_pappers_verified?: number;
  items_pappers_rejected?: number;
  pappers_api_calls?: number;
  error_message?: string;
  started_at: string;
  completed_at?: string;
}

export const useScrapingStatus = () => {
  const [logs, setLogs] = useState<ScrapingLog[]>([]);
  const [loading, setLoading] = useState(true);
  const [isScrapingRunning, setIsScrapingRunning] = useState(false);
  const { toast } = useToast();
  const channelRef = useRef<any>(null);

  const fetchLogs = async () => {
    try {
      console.log('üîÑ Tentative de r√©cup√©ration des logs de scraping...');
      
      const { data, error } = await supabase
        .from('scraping_logs')
        .select('*')
        .order('started_at', { ascending: false })
        .limit(10);

      if (error) {
        console.error('‚ùå Erreur lors de la r√©cup√©ration des logs:', error);
        toast({
          title: "Erreur de connexion",
          description: "Impossible de r√©cup√©rer les logs de scraping. V√©rifiez la connexion √† la base de donn√©es.",
          variant: "destructive"
        });
        throw error;
      }

      console.log('‚úÖ Logs r√©cup√©r√©s avec succ√®s:', data?.length || 0, 'entr√©es');

      const typedLogs: ScrapingLog[] = (data || []).map(log => ({
        ...log,
        status: log.status as 'running' | 'completed' | 'failed'
      }));

      setLogs(typedLogs);
      setIsScrapingRunning(typedLogs.some(log => log.status === 'running') || false);
    } catch (error) {
      console.error('üí• Erreur compl√®te fetchLogs:', error);
      setLogs([]);
      setIsScrapingRunning(false);
      
      // Test de connectivit√©
      try {
        const { data: testData } = await supabase.from('profiles').select('count').single();
        console.log('‚úÖ Test de connectivit√© r√©ussi');
      } catch (testError) {
        console.error('‚ùå Test de connectivit√© √©chou√©:', testError);
        toast({
          title: "Probl√®me de connexion",
          description: "La connexion √† Supabase semble d√©faillante. Rechargez la page.",
          variant: "destructive"
        });
      }
    } finally {
      setLoading(false);
    }
  };

  const startScraping = async (source: string, testMode: boolean = false, departmentCode: string | null = null) => {
    try {
      console.log(`üöÄ D√©marrage du scraping ${testMode ? 'TEST' : 'MASSIF'} pour: ${source}${departmentCode ? ` - D√©partement: ${departmentCode}` : ''}`);
      
      const { data, error } = await supabase.functions.invoke('scrape-repairers', {
        body: { 
          source, 
          testMode,
          departmentCode 
        }
      });

      if (error) {
        console.error('‚ùå Erreur Edge Function:', error);
        throw error;
      }

      console.log('‚úÖ R√©ponse Edge Function:', data);

      const scrapingType = testMode ? "üß™ Test" : "üöÄ Scraping MASSIF";
      const locationText = departmentCode ? ` (D√©partement ${departmentCode})` : " (Toute la France)";

      toast({
        title: `${scrapingType} d√©marr√©`,
        description: `${scrapingType} de ${source}${locationText} lanc√©. ${data?.classification_method ? `M√©thode: ${data.classification_method}` : ''}`,
      });

      // Rafra√Æchir les logs apr√®s un d√©lai
      setTimeout(fetchLogs, 2000);
      
      return data;
    } catch (error) {
      console.error('üí• Erreur start scraping:', error);
      
      toast({
        title: "‚ùå Erreur de scraping",
        description: error.message || "Impossible de d√©marrer le scraping. V√©rifiez les logs.",
        variant: "destructive"
      });
      
      throw error;
    }
  };

  useEffect(() => {
    // Nettoyer toute subscription existante avant d'en cr√©er une nouvelle
    if (channelRef.current) {
      console.log('üßπ Nettoyage de la subscription existante');
      supabase.removeChannel(channelRef.current);
      channelRef.current = null;
    }

    // Charger les logs initiaux
    fetchLogs();

    // Cr√©er une nouvelle subscription uniquement si on n'en a pas d√©j√† une
    if (!channelRef.current) {
      console.log('üì° Cr√©ation de la subscription realtime');
      
      channelRef.current = supabase
        .channel(`scraping_logs_${Date.now()}`) // Nom unique pour √©viter les conflits
        .on('postgres_changes', {
          event: '*',
          schema: 'public',
          table: 'scraping_logs'
        }, (payload) => {
          console.log('üîÑ Changement temps r√©el d√©tect√©:', payload);
          fetchLogs();
        });

      // S'abonner au canal
      channelRef.current.subscribe((status: string) => {
        console.log('üì° Statut subscription:', status);
        if (status === 'SUBSCRIBED') {
          console.log('‚úÖ Subscription realtime active');
        } else if (status === 'CHANNEL_ERROR') {
          console.error('‚ùå Erreur de subscription realtime');
        }
      });
    }

    return () => {
      console.log('üßπ Nettoyage de la subscription lors du d√©montage');
      if (channelRef.current) {
        supabase.removeChannel(channelRef.current);
        channelRef.current = null;
      }
    };
  }, []); // D√©pendances vides pour ne s'ex√©cuter qu'une seule fois

  return {
    logs,
    loading,
    isScrapingRunning,
    startScraping,
    refetch: fetchLogs
  };
};
