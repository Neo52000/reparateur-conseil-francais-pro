
import { useState, useEffect } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useToast } from '@/hooks/use-toast';

export interface ScrapingLog {
  id: string;
  source: string;
  status: 'running' | 'completed' | 'failed';
  items_scraped: number;
  items_added: number;
  items_updated: number;
  error_message?: string;
  started_at: string;
  completed_at?: string;
}

export const useScrapingStatus = () => {
  const [logs, setLogs] = useState<ScrapingLog[]>([]);
  const [loading, setLoading] = useState(true);
  const [isScrapingRunning, setIsScrapingRunning] = useState(false);
  const { toast } = useToast();

  const fetchLogs = async () => {
    try {
      const { data, error } = await supabase
        .from('scraping_logs')
        .select('*')
        .order('started_at', { ascending: false })
        .limit(10);

      if (error) throw error;

      // Cast the data to our ScrapingLog type, ensuring status is properly typed
      const typedLogs: ScrapingLog[] = (data || []).map(log => ({
        ...log,
        status: log.status as 'running' | 'completed' | 'failed'
      }));

      setLogs(typedLogs);
      setIsScrapingRunning(typedLogs.some(log => log.status === 'running') || false);
    } catch (error) {
      console.error('Error fetching scraping logs:', error);
      setLogs([]);
      setIsScrapingRunning(false);
    } finally {
      setLoading(false);
    }
  };

  const startScraping = async (source: string) => {
    try {
      console.log(`ðŸš€ DÃ©marrage du scraping pour: ${source}`);
      
      const { data, error } = await supabase.functions.invoke('scrape-repairers', {
        body: { source }
      });

      if (error) {
        console.error('âŒ Erreur Edge Function:', error);
        throw error;
      }

      console.log('âœ… RÃ©ponse Edge Function:', data);

      toast({
        title: "âœ… Scraping dÃ©marrÃ©",
        description: `Le scraping de ${source} a Ã©tÃ© lancÃ© avec succÃ¨s. ${data?.ai_provider ? `IA utilisÃ©e: ${data.ai_provider}` : ''}`,
      });

      // RafraÃ®chir les logs aprÃ¨s un dÃ©lai
      setTimeout(fetchLogs, 2000);
      
      return data;
    } catch (error) {
      console.error('ðŸ’¥ Erreur start scraping:', error);
      
      toast({
        title: "âŒ Erreur de scraping",
        description: error.message || "Impossible de dÃ©marrer le scraping. VÃ©rifiez les logs.",
        variant: "destructive"
      });
      
      throw error;
    }
  };

  useEffect(() => {
    fetchLogs();

    // Ã‰couter les changements en temps rÃ©el
    const subscription = supabase
      .channel('scraping_logs_realtime')
      .on('postgres_changes', {
        event: '*',
        schema: 'public',
        table: 'scraping_logs'
      }, (payload) => {
        console.log('ðŸ”„ Changement temps rÃ©el dÃ©tectÃ©:', payload);
        fetchLogs();
      })
      .subscribe();

    return () => {
      subscription.unsubscribe();
    };
  }, []);

  return {
    logs,
    loading,
    isScrapingRunning,
    startScraping,
    refetch: fetchLogs
  };
};
